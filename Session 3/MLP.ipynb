{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "INFO:tensorflow:Disabling eager execution\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._X = tf.compat.v1.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.compat.v1.placeholder(tf.int32, shape=[None,])\n",
    "\n",
    "        weights_1 = tf.compat.v1.get_variable(\n",
    "            name='weights_input_hidden',\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "        biases_1 = tf.compat.v1.get_variable(\n",
    "            name='biases_input_hidden',\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021)\n",
    "        )\n",
    "\n",
    "        weights_2 = tf.compat.v1.get_variable(\n",
    "            name='weights_output_hidden',\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "        biases_2 = tf.compat.v1.get_variable(\n",
    "            name='biases_output_hidden',\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021)\n",
    "        )\n",
    "\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "\n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "\n",
    "        return predicted_labels, loss\n",
    "\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope(),reuse=tf.compat.v1.AUTO_REUSE): \n",
    "            train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "            \n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0   \n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "\n",
    "        if end + self._batch_size > len(self._data):\n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2021)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        return self._data[start:end], self._labels[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_data_reader = DataReader(\n",
    "        data_path='../datasets/20news-bydate/20news-train-tfidf.txt',\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    test_data_reader = DataReader(\n",
    "        data_path='../datasets/20news-bydate/20news-test-tfidf.txt',\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = ','.join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
    "    \n",
    "    with open('./saved-paras/' + filename, 'w') as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open('./saved-paras/' + filename, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72139e-08\n",
      "Step: 19514. Loss: 5.7220425730974966e-08\n",
      "Step: 19515. Loss: 3.3378452712895523e-07\n",
      "Step: 19516. Loss: 4.768367389829109e-08\n",
      "Step: 19517. Loss: 1.506749299551302e-06\n",
      "Step: 19518. Loss: 3.099441059362107e-08\n",
      "Step: 19519. Loss: 6.198876434382328e-08\n",
      "Step: 19520. Loss: 1.7404494201400666e-07\n",
      "Step: 19521. Loss: 1.192092646817855e-08\n",
      "Step: 19522. Loss: 1.811978904697753e-07\n",
      "Step: 19523. Loss: 0.00017130190099123865\n",
      "Step: 19524. Loss: 1.430511176181426e-08\n",
      "Step: 19525. Loss: 1.4066682751945336e-07\n",
      "Step: 19526. Loss: 3.0134581265883753e-06\n",
      "Step: 19527. Loss: 7.390973166820913e-08\n",
      "Step: 19528. Loss: 5.316686610967736e-07\n",
      "Step: 19529. Loss: 2.6009788598457817e-06\n",
      "Step: 19530. Loss: 3.943071533285547e-06\n",
      "Step: 19531. Loss: 2.074236391536033e-07\n",
      "Step: 19532. Loss: 0.16136722266674042\n",
      "Step: 19533. Loss: 1.0514010000406415e-06\n",
      "Step: 19534. Loss: 7.629385123664179e-08\n",
      "Step: 19535. Loss: 1.2397752868764655e-07\n",
      "Step: 19536. Loss: 4.529947972287118e-08\n",
      "Step: 19537. Loss: 3.576278118089249e-08\n",
      "Step: 19538. Loss: 9.298316427930331e-08\n",
      "Step: 19539. Loss: 0.0\n",
      "Step: 19540. Loss: 3.4853810575441457e-06\n",
      "Step: 19541. Loss: 5.722042217826129e-08\n",
      "Step: 19542. Loss: 1.215932599052394e-07\n",
      "Step: 19543. Loss: 9.059896655116972e-08\n",
      "Step: 19544. Loss: 2.5749071141945024e-07\n",
      "Step: 19545. Loss: 4.935207016387722e-07\n",
      "Step: 19546. Loss: 1.7404491359229723e-07\n",
      "Step: 19547. Loss: 3.337853513585287e-07\n",
      "Step: 19548. Loss: 7.15255676908555e-09\n",
      "Step: 19549. Loss: 2.348320549572236e-06\n",
      "Step: 19550. Loss: 1.430511176181426e-08\n",
      "Step: 19551. Loss: 2.2649719255696255e-07\n",
      "Step: 19552. Loss: 9.059888128604143e-08\n",
      "Step: 19553. Loss: 1.2397752868764655e-07\n",
      "Step: 19554. Loss: 1.0251990545384615e-07\n",
      "Step: 19555. Loss: 8.368343173970061e-07\n",
      "Step: 19556. Loss: 0.057879164814949036\n",
      "Step: 19557. Loss: 2.5749122301022e-07\n",
      "Step: 19558. Loss: 4.076944719599851e-07\n",
      "Step: 19559. Loss: 1.0728813037985674e-07\n",
      "Step: 19560. Loss: 3.6189533147990005e-06\n",
      "Step: 19561. Loss: 5.602757937595015e-07\n",
      "Step: 19562. Loss: 7.796204499754822e-07\n",
      "Step: 19563. Loss: 1.573561405621149e-07\n",
      "Step: 19564. Loss: 3.7261365832819138e-06\n",
      "Step: 19565. Loss: 3.337859411089994e-08\n",
      "Step: 19566. Loss: 2.38418529363571e-08\n",
      "Step: 19567. Loss: 0.13909406960010529\n",
      "Step: 19568. Loss: 4.76837103136063e-09\n",
      "Step: 19569. Loss: 1.1444076619682164e-07\n",
      "Step: 19570. Loss: 7.15255676908555e-09\n",
      "Step: 19571. Loss: 2.3364920309631998e-07\n",
      "Step: 19572. Loss: 5.7220432836402324e-08\n",
      "Step: 19573. Loss: 2.2649699360499653e-07\n",
      "Step: 19574. Loss: 3.335228029754944e-06\n",
      "Step: 19575. Loss: 3.1850318009674083e-06\n",
      "Step: 19576. Loss: 2.384185116000026e-08\n",
      "Step: 19577. Loss: 3.242473667341983e-07\n",
      "Step: 19578. Loss: 0.1922789216041565\n",
      "Step: 19579. Loss: 1.8119773415037344e-07\n",
      "Step: 19580. Loss: 1.5497160177346814e-07\n",
      "Step: 19581. Loss: 2.145766764272139e-08\n",
      "Step: 19582. Loss: 2.4795451736281393e-07\n",
      "Step: 19583. Loss: 2.145766586636455e-08\n",
      "Step: 19584. Loss: 3.4711422358668642e-06\n",
      "Step: 19585. Loss: 2.3126541748297313e-07\n",
      "Step: 19586. Loss: 1.430511176181426e-08\n",
      "Step: 19587. Loss: 2.908700196257996e-07\n",
      "Step: 19588. Loss: 3.0994396382766354e-08\n",
      "Step: 19589. Loss: 7.15255676908555e-09\n",
      "Step: 19590. Loss: 2.38418573772492e-09\n",
      "Step: 19591. Loss: 3.4925396903418005e-06\n",
      "Step: 19592. Loss: 7.15255632499634e-09\n",
      "Step: 19593. Loss: 5.245204803827619e-08\n",
      "Step: 19594. Loss: 0.07669416069984436\n",
      "Step: 19595. Loss: 1.8119754940926214e-07\n",
      "Step: 19596. Loss: 3.3855286574180354e-07\n",
      "Step: 19597. Loss: 1.0490398238971466e-07\n",
      "Step: 19598. Loss: 2.741801381489495e-07\n",
      "Step: 19599. Loss: 2.193447130593995e-07\n",
      "Step: 19600. Loss: 3.1636293442716124e-06\n",
      "Step: 19601. Loss: 9.56041731114965e-07\n",
      "Step: 19602. Loss: 1.907348412544252e-08\n",
      "Step: 19603. Loss: 2.741808202699758e-07\n",
      "Step: 19604. Loss: 2.38418529363571e-08\n",
      "Step: 19605. Loss: 5.769654194409668e-07\n",
      "Step: 19606. Loss: 1.034710635394731e-06\n",
      "Step: 19607. Loss: 2.908689680225507e-07\n",
      "Step: 19608. Loss: 4.374531727080466e-06\n",
      "Step: 19609. Loss: 8.10608526080614e-07\n",
      "Step: 19610. Loss: 3.471339005045593e-05\n",
      "Step: 19611. Loss: 0.10007575154304504\n",
      "Step: 19612. Loss: 1.7881353642223985e-07\n",
      "Step: 19613. Loss: 0.23518812656402588\n",
      "Step: 19614. Loss: 7.057109314700938e-07\n",
      "Step: 19615. Loss: 5.0067875179138355e-08\n",
      "Step: 19616. Loss: 7.390962508679877e-08\n",
      "Step: 19617. Loss: 1.5974013933828246e-07\n",
      "Step: 19618. Loss: 3.5047250435127353e-07\n",
      "Step: 19619. Loss: 0.10176685452461243\n",
      "Step: 19620. Loss: 3.1471012107431307e-07\n",
      "Step: 19621. Loss: 4.2199710037493787e-07\n",
      "Step: 19622. Loss: 1.5729538063169457e-05\n",
      "Step: 19623. Loss: 5.006789294270675e-08\n",
      "Step: 19624. Loss: 6.198879276553271e-08\n",
      "Step: 19625. Loss: 5.483622800284138e-08\n",
      "Step: 19626. Loss: 7.15255632499634e-09\n",
      "Step: 19627. Loss: 6.437293365024743e-08\n",
      "Step: 19628. Loss: 1.2222383702464867e-05\n",
      "Step: 19629. Loss: 4.76837147544984e-09\n",
      "Step: 19630. Loss: 1.192092646817855e-08\n",
      "Step: 19631. Loss: 2.021694626819226e-06\n",
      "Step: 19632. Loss: 1.192092824453539e-08\n",
      "Step: 19633. Loss: 2.1457660537294032e-08\n",
      "Step: 19634. Loss: 7.152553394007555e-08\n",
      "Step: 19635. Loss: 2.0027079017381766e-07\n",
      "Step: 19636. Loss: 1.2397744342251826e-07\n",
      "Step: 19637. Loss: 1.3351427696761675e-07\n",
      "Step: 19638. Loss: 3.099440348819371e-08\n",
      "Step: 19639. Loss: 0.09176767617464066\n",
      "Step: 19640. Loss: 2.38418573772492e-09\n",
      "Step: 19641. Loss: 4.76837147544984e-09\n",
      "Step: 19642. Loss: 1.430507978739115e-07\n",
      "Step: 19643. Loss: 9.536731937487275e-08\n",
      "Step: 19644. Loss: 2.837166448443895e-07\n",
      "Step: 19645. Loss: 4.768368100371845e-08\n",
      "Step: 19646. Loss: 2.2411281008771766e-07\n",
      "Step: 19647. Loss: 1.1444080172395843e-07\n",
      "Step: 19648. Loss: 2.694121974400332e-07\n",
      "Step: 19649. Loss: 5.722045059997072e-08\n",
      "Step: 19650. Loss: 9.775147447044219e-08\n",
      "Step: 19651. Loss: 1.668929527909313e-08\n",
      "Step: 19652. Loss: 2.622603467727913e-08\n",
      "Step: 19653. Loss: 5.9604580826544407e-08\n",
      "Step: 19654. Loss: 4.76837103136063e-09\n",
      "Step: 19655. Loss: 1.668929705544997e-08\n",
      "Step: 19656. Loss: 0.039139244705438614\n",
      "Step: 19657. Loss: 0.0\n",
      "Step: 19658. Loss: 3.790836444750312e-07\n",
      "Step: 19659. Loss: 6.437291943939272e-08\n",
      "Step: 19660. Loss: 5.4836217344700344e-08\n",
      "Step: 19661. Loss: 1.6450330804218538e-06\n",
      "Step: 19662. Loss: 9.387503610014392e-07\n",
      "Step: 19663. Loss: 1.668929883180681e-08\n",
      "Step: 19664. Loss: 2.9802194490002876e-07\n",
      "Step: 19665. Loss: 1.8119767730695457e-07\n",
      "Step: 19666. Loss: 0.046640824526548386\n",
      "Step: 19667. Loss: 8.583051425148369e-08\n",
      "Step: 19668. Loss: 1.7999856254391489e-06\n",
      "Step: 19669. Loss: 5.115857220516773e-06\n",
      "Step: 19670. Loss: 6.198880697638742e-08\n",
      "Step: 19671. Loss: 2.5749082510628796e-07\n",
      "Step: 19672. Loss: 4.5299508144580614e-08\n",
      "Step: 19673. Loss: 1.504367673987872e-06\n",
      "Step: 19674. Loss: 2.670280139227543e-07\n",
      "Step: 19675. Loss: 4.935209858558665e-07\n",
      "Step: 19676. Loss: 5.269016583042685e-07\n",
      "Step: 19677. Loss: 5.722038309841082e-08\n",
      "Step: 19678. Loss: 9.369658755531418e-07\n",
      "Step: 19679. Loss: 1.1682502787380145e-07\n",
      "Step: 19680. Loss: 2.837167016878084e-07\n",
      "Step: 19681. Loss: 1.568774337101786e-06\n",
      "Step: 19682. Loss: 3.814677995706006e-07\n",
      "Step: 19683. Loss: 5.0494645620347e-06\n",
      "Step: 19684. Loss: 4.7416378947673365e-06\n",
      "Step: 19685. Loss: 6.437261390601634e-07\n",
      "Step: 19686. Loss: 2.574911661668011e-07\n",
      "Step: 19687. Loss: 1.5258761720815528e-07\n",
      "Step: 19688. Loss: 1.1205658978497013e-07\n",
      "Step: 19689. Loss: 0.08500701934099197\n",
      "Step: 19690. Loss: 4.0531134004595515e-08\n",
      "Step: 19691. Loss: 7.988603101694025e-06\n",
      "Step: 19692. Loss: 3.073012067034142e-06\n",
      "Step: 19693. Loss: 1.668927183118285e-07\n",
      "Step: 19694. Loss: 1.0013572193656728e-07\n",
      "Step: 19695. Loss: 4.76837147544984e-09\n",
      "Step: 19696. Loss: 3.004056452482473e-07\n",
      "Step: 19697. Loss: 1.3828238820678962e-07\n",
      "Step: 19698. Loss: 5.006789294270675e-08\n",
      "Step: 19699. Loss: 6.437296917738422e-08\n",
      "Step: 19700. Loss: 5.960462345910855e-08\n",
      "Step: 19701. Loss: 3.4570447837722895e-07\n",
      "Step: 19702. Loss: 6.198879276553271e-08\n",
      "Step: 19703. Loss: 8.344639468305104e-08\n",
      "Step: 19704. Loss: 1.0490395396800523e-07\n",
      "Step: 19705. Loss: 2.455703622672445e-07\n",
      "Step: 19706. Loss: 3.4472436709620524e-06\n",
      "Step: 19707. Loss: 3.8146957592744e-08\n",
      "Step: 19708. Loss: 6.437293365024743e-08\n",
      "Step: 19709. Loss: 6.866368948976742e-07\n",
      "Step: 19710. Loss: 2.0265547107101156e-07\n",
      "Step: 19711. Loss: 1.0394799119239906e-06\n",
      "Step: 19712. Loss: 3.1945646696840413e-06\n",
      "Step: 19713. Loss: 3.099440348819371e-08\n",
      "Step: 19714. Loss: 4.0531130451881836e-08\n",
      "Step: 19715. Loss: 1.3113007923948317e-07\n",
      "Step: 19716. Loss: 8.583059951661198e-08\n",
      "Step: 19717. Loss: 6.67571669055178e-08\n",
      "Step: 19718. Loss: 7.104758878995199e-07\n",
      "Step: 19719. Loss: 4.291495940833556e-07\n",
      "Step: 19720. Loss: 3.0517523441631056e-07\n",
      "Step: 19721. Loss: 2.026553147516097e-07\n",
      "Step: 19722. Loss: 2.121920630315799e-07\n",
      "Step: 19723. Loss: 1.6927666024457722e-07\n",
      "Step: 19724. Loss: 5.006753553971066e-07\n",
      "Step: 19725. Loss: 1.668929527909313e-08\n",
      "Step: 19726. Loss: 0.05991823226213455\n",
      "Step: 19727. Loss: 7.152548420208404e-08\n",
      "Step: 19728. Loss: 7.152551972922083e-08\n",
      "Step: 19729. Loss: 1.1467765261841123e-06\n",
      "Step: 19730. Loss: 2.074238381055693e-07\n",
      "Step: 19731. Loss: 3.719316055139643e-07\n",
      "Step: 19732. Loss: 5.912696678933571e-07\n",
      "Step: 19733. Loss: 3.814696469817136e-08\n",
      "Step: 19734. Loss: 4.1961334318330046e-07\n",
      "Step: 19735. Loss: 2.2411306588310254e-07\n",
      "Step: 19736. Loss: 6.270324206525402e-07\n",
      "Step: 19737. Loss: 5.006785741556996e-08\n",
      "Step: 19738. Loss: 2.9086911013109784e-07\n",
      "Step: 19739. Loss: 9.298312164673916e-08\n",
      "Step: 19740. Loss: 2.6438895019964548e-06\n",
      "Step: 19741. Loss: 1.3589823311122018e-07\n",
      "Step: 19742. Loss: 2.6226025795494934e-08\n",
      "Step: 19743. Loss: 2.9325286732273526e-07\n",
      "Step: 19744. Loss: 1.6641126876493217e-06\n",
      "Step: 19745. Loss: 4.76837147544984e-09\n",
      "Step: 19746. Loss: 1.907342692675229e-07\n",
      "Step: 19747. Loss: 1.0251991966470086e-07\n",
      "Step: 19748. Loss: 1.7594763903616695e-06\n",
      "Step: 19749. Loss: 1.9073424084581347e-07\n",
      "Step: 19750. Loss: 4.76837103136063e-09\n",
      "Step: 19751. Loss: 7.15255676908555e-09\n",
      "Step: 19752. Loss: 3.6832423120358726e-06\n",
      "Step: 19753. Loss: 5.611725100607146e-06\n",
      "Step: 19754. Loss: 4.863690037382185e-07\n",
      "Step: 19755. Loss: 0.0\n",
      "Step: 19756. Loss: 1.0037176707555773e-06\n",
      "Step: 19757. Loss: 5.721998377339332e-07\n",
      "Step: 19758. Loss: 3.6477908338383713e-07\n",
      "Step: 19759. Loss: 6.176506303745555e-06\n",
      "Step: 19760. Loss: 2.121918782904686e-07\n",
      "Step: 19761. Loss: 5.006771743865102e-07\n",
      "Step: 19762. Loss: 5.078289859739016e-07\n",
      "Step: 19763. Loss: 3.6954799043087405e-07\n",
      "Step: 19764. Loss: 6.437296207195686e-08\n",
      "Step: 19765. Loss: 0.13359269499778748\n",
      "Step: 19766. Loss: 5.960459503739912e-08\n",
      "Step: 19767. Loss: 2.417434643575689e-06\n",
      "Step: 19768. Loss: 2.622604000634965e-08\n",
      "Step: 19769. Loss: 3.4332100540268584e-07\n",
      "Step: 19770. Loss: 2.622604000634965e-08\n",
      "Step: 19771. Loss: 2.408022510280716e-07\n",
      "Step: 19772. Loss: 6.675714558923573e-08\n",
      "Step: 19773. Loss: 1.192092824453539e-08\n",
      "Step: 19774. Loss: 1.52587531943027e-07\n",
      "Step: 19775. Loss: 5.722043994182968e-08\n",
      "Step: 19776. Loss: 3.099441059362107e-08\n",
      "Step: 19777. Loss: 1.0490403212770616e-07\n",
      "Step: 19778. Loss: 0.36096322536468506\n",
      "Step: 19779. Loss: 2.622598458401626e-07\n",
      "Step: 19780. Loss: 1.168249852412373e-07\n",
      "Step: 19781. Loss: 2.38418573772492e-09\n",
      "Step: 19782. Loss: 1.907348234908568e-08\n",
      "Step: 19783. Loss: 1.430511264999268e-08\n",
      "Step: 19784. Loss: 2.9563724979198014e-07\n",
      "Step: 19785. Loss: 2.3364927415059356e-07\n",
      "Step: 19786. Loss: 9.298312875216652e-08\n",
      "Step: 19787. Loss: 8.344643021018783e-08\n",
      "Step: 19788. Loss: 1.192092646817855e-08\n",
      "Step: 19789. Loss: 5.245202316928044e-08\n",
      "Step: 19790. Loss: 0.0009145363583229482\n",
      "Step: 19791. Loss: 5.7220436389116e-08\n",
      "Step: 19792. Loss: 2.55107295288326e-07\n",
      "Step: 19793. Loss: 3.576277407546513e-08\n",
      "Step: 19794. Loss: 7.399210062430939e-06\n",
      "Step: 19795. Loss: 1.192092824453539e-08\n",
      "Step: 19796. Loss: 1.883500431176799e-07\n",
      "Step: 19797. Loss: 7.152554815093026e-08\n",
      "Step: 19798. Loss: 3.3378579900045224e-08\n",
      "Step: 19799. Loss: 0.06301815062761307\n",
      "Step: 19800. Loss: 8.010753163034678e-07\n",
      "Step: 19801. Loss: 1.907348412544252e-08\n",
      "Step: 19802. Loss: 1.7404485674887837e-07\n",
      "Step: 19803. Loss: 2.2458034436567686e-06\n",
      "Step: 19804. Loss: 1.335142201241979e-07\n",
      "Step: 19805. Loss: 1.1872896266140742e-06\n",
      "Step: 19806. Loss: 0.004552869591861963\n",
      "Step: 19807. Loss: 3.442479282966815e-06\n",
      "Step: 19808. Loss: 3.3140022992483864e-07\n",
      "Step: 19809. Loss: 7.867802054306594e-08\n",
      "Step: 19810. Loss: 1.2159337359207711e-07\n",
      "Step: 19811. Loss: 6.556417702086037e-07\n",
      "Step: 19812. Loss: 1.6450849216198549e-07\n",
      "Step: 19813. Loss: 9.059900207830651e-08\n",
      "Step: 19814. Loss: 3.576278118089249e-08\n",
      "Step: 19815. Loss: 8.177614745363826e-07\n",
      "Step: 19816. Loss: 7.581595014016784e-07\n",
      "Step: 19817. Loss: 2.1936939447186887e-05\n",
      "Step: 19818. Loss: 0.10672171413898468\n",
      "Step: 19819. Loss: 2.169605011204112e-07\n",
      "Step: 19820. Loss: 5.2452033827421474e-08\n",
      "Step: 19821. Loss: 5.722041507283393e-08\n",
      "Step: 19822. Loss: 0.08227170258760452\n",
      "Step: 19823. Loss: 1.192092824453539e-08\n",
      "Step: 19824. Loss: 1.1205662531210692e-07\n",
      "Step: 19825. Loss: 2.38418529363571e-08\n",
      "Step: 19826. Loss: 6.675713137838102e-08\n",
      "Step: 19827. Loss: 3.528581942191522e-07\n",
      "Step: 19828. Loss: 1.1682497813580994e-07\n",
      "Step: 19829. Loss: 2.574905977326125e-07\n",
      "Step: 19830. Loss: 7.629388676377857e-08\n",
      "Step: 19831. Loss: 5.888875875825761e-07\n",
      "Step: 19832. Loss: 3.194788575910934e-07\n",
      "Step: 19833. Loss: 8.106222537662688e-08\n",
      "Step: 19834. Loss: 1.430511264999268e-08\n",
      "Step: 19835. Loss: 5.833627710671863e-06\n",
      "Step: 19836. Loss: 1.8358151976372028e-07\n",
      "Step: 19837. Loss: 1.2778967857229873e-06\n",
      "Step: 19838. Loss: 3.814693627646193e-08\n",
      "Step: 19839. Loss: 0.04254225268959999\n",
      "Step: 19840. Loss: 1.430511264999268e-08\n",
      "Step: 19841. Loss: 8.440661076747347e-06\n",
      "Step: 19842. Loss: 2.4556245534768095e-06\n",
      "Step: 19843. Loss: 8.129913453558402e-07\n",
      "Step: 19844. Loss: 5.7220425730974966e-08\n",
      "Step: 19845. Loss: 1.7404501306828024e-07\n",
      "Step: 19846. Loss: 2.2411282429857238e-07\n",
      "Step: 19847. Loss: 0.06880812346935272\n",
      "Step: 19848. Loss: 1.8524285678722663e-06\n",
      "Step: 19849. Loss: 4.2915321074588064e-08\n",
      "Step: 19850. Loss: 0.0\n",
      "Step: 19851. Loss: 2.0027077596296294e-07\n",
      "Step: 19852. Loss: 5.0067875179138355e-08\n",
      "Step: 19853. Loss: 1.1444076619682164e-07\n",
      "Step: 19854. Loss: 8.511364057994797e-07\n",
      "Step: 19855. Loss: 1.907348234908568e-08\n",
      "Step: 19856. Loss: 3.099439993548003e-08\n",
      "Step: 19857. Loss: 5.722042217826129e-08\n",
      "Step: 19858. Loss: 3.814696469817136e-08\n",
      "Step: 19859. Loss: 1.4543488191520737e-07\n",
      "Step: 19860. Loss: 0.11687017232179642\n",
      "Step: 19861. Loss: 8.106221827119953e-08\n",
      "Step: 19862. Loss: 1.4829099654889433e-06\n",
      "Step: 19863. Loss: 3.33785834527589e-08\n",
      "Step: 19864. Loss: 6.914132200108725e-08\n",
      "Step: 19865. Loss: 3.337858700547258e-08\n",
      "Step: 19866. Loss: 6.675715980009045e-08\n",
      "Step: 19867. Loss: 2.982493924719165e-06\n",
      "Step: 19868. Loss: 2.7001879061572254e-05\n",
      "Step: 19869. Loss: 3.230312813684577e-06\n",
      "Step: 19870. Loss: 1.192092646817855e-08\n",
      "Step: 19871. Loss: 4.76837147544984e-09\n",
      "Step: 19872. Loss: 9.965685876522912e-07\n",
      "Step: 19873. Loss: 0.048651404678821564\n",
      "Step: 19874. Loss: 1.4066678488688922e-07\n",
      "Step: 19875. Loss: 2.1457660537294032e-08\n",
      "Step: 19876. Loss: 2.145766586636455e-08\n",
      "Step: 19877. Loss: 9.059900207830651e-08\n",
      "Step: 19878. Loss: 2.1624091459671035e-06\n",
      "Step: 19879. Loss: 1.52587531943027e-07\n",
      "Step: 19880. Loss: 2.991954943354358e-06\n",
      "Step: 19881. Loss: 1.883420736703556e-06\n",
      "Step: 19882. Loss: 1.26361683783216e-07\n",
      "Step: 19883. Loss: 0.12099915742874146\n",
      "Step: 19884. Loss: 1.192090977042426e-07\n",
      "Step: 19885. Loss: 2.551063573719148e-07\n",
      "Step: 19886. Loss: 1.0728825117212182e-07\n",
      "Step: 19887. Loss: 7.557731578344828e-07\n",
      "Step: 19888. Loss: 5.587935003603661e-09\n",
      "Step: 19889. Loss: 2.503389566754777e-07\n",
      "Step: 19890. Loss: 5.245206580184458e-08\n",
      "Step: 19891. Loss: 2.884847560835624e-07\n",
      "Step: 19892. Loss: 3.576277762817881e-08\n",
      "Step: 19893. Loss: 8.583057820032991e-08\n",
      "Step: 19894. Loss: 2.3126503378989582e-07\n",
      "Step: 19895. Loss: 2.4318572400261473e-07\n",
      "Step: 19896. Loss: 5.006788228456571e-08\n",
      "Step: 19897. Loss: 2.400746097919182e-06\n",
      "Step: 19898. Loss: 7.15255676908555e-09\n",
      "Step: 19899. Loss: 1.668929883180681e-08\n",
      "Step: 19900. Loss: 9.012130703922594e-07\n",
      "Step: 19901. Loss: 1.025198130832905e-07\n",
      "Step: 19902. Loss: 3.8000484892108943e-06\n",
      "Step: 19903. Loss: 1.1920906928253316e-07\n",
      "Step: 19904. Loss: 0.0025846606586128473\n",
      "Step: 19905. Loss: 1.4781466006752453e-06\n",
      "Step: 19906. Loss: 3.883469162246911e-06\n",
      "Step: 19907. Loss: 1.8476679315426736e-06\n",
      "Step: 19908. Loss: 1.0704757187340874e-06\n",
      "Step: 19909. Loss: 4.76837103136063e-09\n",
      "Step: 19910. Loss: 3.719314065619983e-07\n",
      "Step: 19911. Loss: 1.430511264999268e-08\n",
      "Step: 19912. Loss: 1.8634549633134156e-05\n",
      "Step: 19913. Loss: 1.907348234908568e-08\n",
      "Step: 19914. Loss: 3.0279122142928827e-07\n",
      "Step: 19915. Loss: 6.198879276553271e-08\n",
      "Step: 19916. Loss: 0.08092945069074631\n",
      "Step: 19917. Loss: 8.106225379833631e-08\n",
      "Step: 19918. Loss: 7.15255632499634e-09\n",
      "Step: 19919. Loss: 1.3351426275676204e-07\n",
      "Step: 19920. Loss: 2.38418573772492e-09\n",
      "Step: 19921. Loss: 1.573557710798923e-07\n",
      "Step: 19922. Loss: 2.431865198104788e-07\n",
      "Step: 19923. Loss: 5.960457727383073e-08\n",
      "Step: 19924. Loss: 2.4746329927438637e-06\n",
      "Step: 19925. Loss: 9.53674206272126e-09\n",
      "Step: 19926. Loss: 8.344639468305104e-08\n",
      "Step: 19927. Loss: 6.198877855467799e-08\n",
      "Step: 19928. Loss: 2.861022352362852e-08\n",
      "Step: 19929. Loss: 9.536724121517182e-08\n",
      "Step: 19930. Loss: 1.5043702887851396e-06\n",
      "Step: 19931. Loss: 1.0251984150499993e-07\n",
      "Step: 19932. Loss: 9.53673620074369e-08\n",
      "Step: 19933. Loss: 6.699461891912506e-07\n",
      "Step: 19934. Loss: 5.2452058696417225e-08\n",
      "Step: 19935. Loss: 2.6226025795494934e-08\n",
      "Step: 19936. Loss: 0.0026856679469347\n",
      "Step: 19937. Loss: 8.511410101164074e-07\n",
      "Step: 19938. Loss: 7.390971745735442e-08\n",
      "Step: 19939. Loss: 8.249213010458334e-07\n",
      "Step: 19940. Loss: 0.12005230039358139\n",
      "Step: 19941. Loss: 1.4095162441662978e-05\n",
      "Step: 19942. Loss: 1.8762876834443887e-06\n",
      "Step: 19943. Loss: 1.8358194608936174e-07\n",
      "Step: 19944. Loss: 4.76837103136063e-09\n",
      "Step: 19945. Loss: 0.022784046828746796\n",
      "Step: 19946. Loss: 1.668929350273629e-08\n",
      "Step: 19947. Loss: 4.768369876728684e-08\n",
      "Step: 19948. Loss: 5.245206580184458e-08\n",
      "Step: 19949. Loss: 3.838517272924946e-07\n",
      "Step: 19950. Loss: 2.457985146975261e-06\n",
      "Step: 19951. Loss: 1.8358207398705417e-07\n",
      "Step: 19952. Loss: 0.0600200891494751\n",
      "Step: 19953. Loss: 1.430510998545742e-08\n",
      "Step: 19954. Loss: 1.4305075524134736e-07\n",
      "Step: 19955. Loss: 2.932534926003427e-07\n",
      "Step: 19956. Loss: 7.15255632499634e-09\n",
      "Step: 19957. Loss: 1.907347702001516e-08\n",
      "Step: 19958. Loss: 5.245205159098987e-08\n",
      "Step: 19959. Loss: 4.2676475686675985e-07\n",
      "Step: 19960. Loss: 4.672493105317699e-06\n",
      "Step: 19961. Loss: 1.3113006502862845e-07\n",
      "Step: 19962. Loss: 5.030597662880609e-07\n",
      "Step: 19963. Loss: 4.053114821545023e-08\n",
      "Step: 19964. Loss: 4.76837147544984e-09\n",
      "Step: 19965. Loss: 0.00010414457938168198\n",
      "Step: 19966. Loss: 7.390973166820913e-08\n",
      "Step: 19967. Loss: 1.4305099682587752e-07\n",
      "Step: 19968. Loss: 2.760730239970144e-06\n",
      "Step: 19969. Loss: 8.106226090376367e-08\n",
      "Step: 19970. Loss: 4.625293570370559e-07\n",
      "Step: 19971. Loss: 5.96045985901128e-08\n",
      "Step: 19972. Loss: 0.04212731495499611\n",
      "Step: 19973. Loss: 2.0027137281886098e-07\n",
      "Step: 19974. Loss: 6.437296917738422e-08\n",
      "Step: 19975. Loss: 0.20821209251880646\n",
      "Step: 19976. Loss: 2.5272285597566224e-07\n",
      "Step: 19977. Loss: 1.907348412544252e-08\n",
      "Step: 19978. Loss: 7.39097032464997e-08\n",
      "Step: 19979. Loss: 5.650480829899607e-07\n",
      "Step: 19980. Loss: 3.766998304399749e-07\n",
      "Step: 19981. Loss: 1.430511264999268e-08\n",
      "Step: 19982. Loss: 1.2397751447679184e-07\n",
      "Step: 19983. Loss: 1.1205654715240598e-07\n",
      "Step: 19984. Loss: 3.123259659787436e-07\n",
      "Step: 19985. Loss: 5.340505140338792e-07\n",
      "Step: 19986. Loss: 5.245206935455826e-08\n",
      "Step: 19987. Loss: 4.546136551653035e-06\n",
      "Step: 19988. Loss: 3.576278118089249e-08\n",
      "Step: 19989. Loss: 6.437296207195686e-08\n",
      "Step: 19990. Loss: 4.768369876728684e-08\n",
      "Step: 19991. Loss: 2.622603467727913e-08\n",
      "Step: 19992. Loss: 2.145766764272139e-08\n",
      "Step: 19993. Loss: 2.861012262656004e-07\n",
      "Step: 19994. Loss: 1.907348412544252e-08\n",
      "Step: 19995. Loss: 0.039407018572092056\n",
      "Step: 19996. Loss: 6.460118584072916e-06\n",
      "Step: 19997. Loss: 4.76837147544984e-09\n",
      "Step: 19998. Loss: 7.670842933293898e-06\n",
      "Step: 19999. Loss: 2.3603308818564983e-07\n",
      "Step: 20000. Loss: 1.430511264999268e-08\n",
      "Epoch: 88\n",
      "Accuracy on test data: 0.7380509824747743\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/20news-bydate/words_idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "\n",
    "mlp = MLP(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=50\n",
    ")\n",
    "\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss=loss, learning_rate=0.1)\n",
    "\n",
    "train_data_reader, test_data_reader = load_dataset()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    step, MAX_STEP = 0, 20000\n",
    "\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        plabels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        step += 1\n",
    "        print('Step: {}. Loss: {}'.format(step, loss_eval))\n",
    "\n",
    "    trainable_variables = tf.compat.v1.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        save_parameters(\n",
    "            name=variable.name,\n",
    "            value=variable.eval(),\n",
    "            epoch=train_data_reader._num_epoch\n",
    "        )\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    epoch = train_data_reader._num_epoch\n",
    "    trainable_variables = tf.compat.v1.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "    \n",
    "    num_true_preds = 0\n",
    "    while True:\n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_plabels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict={\n",
    "                mlp._X: test_data,\n",
    "                mlp._real_Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_plabels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Accuracy on test data: {}'.format(num_true_preds/len(test_data_reader._data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}